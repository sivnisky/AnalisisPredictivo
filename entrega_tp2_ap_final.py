# -*- coding: utf-8 -*-
"""Entrega_TP2_AP_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EQ1sot04Sh_XcLnX0ddue7qd-XqpBkfd

Trabajo Práctico N°2 - Análisis Predictivo - Sofia Ivnisky

# Importación de librerías y acceso a los datos

Importación de librerías
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import lightgbm as lgb
import pandas as pd
import xgboost as xgb
import numpy as np
import json

! pip install lightgbm
from lightgbm import LGBMRegressor
!pip install catboost
from catboost import CatBoostRegressor
!pip install pipeline 
!pip install --upgrade scikit-learn
!pip install feature_engine
!pip install lightgbm 
!pip install optuna

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from feature_engine.preprocessing import MatchVariables
from feature_engine.encoding import RareLabelEncoder, OrdinalEncoder, OneHotEncoder
from feature_engine.imputation import (
    AddMissingIndicator, MeanMedianImputer, CategoricalImputer, EndTailImputer)
from feature_engine.creation import RelativeFeatures
from sklearn.preprocessing import PowerTransformer, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from sklearn.ensemble import GradientBoostingRegressor
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from textblob import TextBlob
from google.colab import drive

"""Acceso al Drive"""

!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

drive.mount('/content/drive', force_remount=True)

"""Acceso al archivo kaggle.json"""

with open("/content/drive/My Drive/ITBA/Análisis Predictivo/kaggle.json", 'r') as f:
    api_token= json.load(f)

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c analisis-predictivo-2023q1


import zipfile
import os

os.listdir()

for file in os.listdir():
    if file.endswith('.zip'):
      zip_ref = zipfile.ZipFile(file, 'r')
      zip_ref.extractall()
      zip_ref.close()

"""Lectura de los datasets"""

df_train = pd.read_csv('/content/base_train.csv')

df_val = pd.read_csv('/content/base_val.csv')

"""# Análisis exploratorio de los datos

Se pide la cantidad de registros y variables
"""

num_registros = len(df_train)
num_variables = len(df_train.columns)
num_registros1 = len(df_val)
num_variables1 = len(df_val.columns)

print("Número de registros df_train:", num_registros)
print("Número de variables df_train:", num_variables)
print("Número de registros df_val:", num_registros)
print("Número de variables df_val:", num_variables)

"""Se visualizan las columnas"""

df_train.columns

df_val.columns

"""Se observa el tipo de datos del dataset"""

df_train.dtypes.unique()

df_val.dtypes.unique()

"""¿Hay alguna fecha mínima de noches mayor a la fecha máxima?"""

df_train[['minimum_nights','maximum_nights']][(df_train['maximum_nights']!=0) & (df_train['maximum_nights'] < df_train['minimum_nights'])]

"""¿Cuántos valores iguales a 0 hay por columna?"""

num_ceros = df_train.eq(0).sum()
print("Valores = 0 por columna:", num_ceros)

"""¿Cuántos missing values hay por columna?"""

print(df_train.isnull().sum())

"""# Análisis estadístico de la variable de interés

Distribución de la variable de interés 'review_scores_rating'
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calcular los valores estadísticos
mean_value = df_train['review_scores_rating'].mean()
median_value = df_train['review_scores_rating'].median()
max_value = df_train['review_scores_rating'].max()
min_value = df_train['review_scores_rating'].min()

plt.figure(figsize=(10, 5))
sns.kdeplot(df_train['review_scores_rating'], bw_adjust=0.5)

# Agregar líneas verticales para los valores estadísticos
plt.axvline(mean_value, color='red', linestyle='--', label='Media')
plt.axvline(median_value, color='green', linestyle='--', label='Mediana')
plt.axvline(max_value, color='orange', linestyle='--', label='Máximo')
plt.axvline(min_value, color='purple', linestyle='--', label='Mínimo')

plt.title('Distribución de review_scores_rating')
plt.xlabel('review_scores_rating')
plt.ylabel('Densidad')

# Mostrar los valores exactos al lado de la leyenda
plt.legend(loc='upper left')
plt.text(1.02, 0.75, f'Media: {mean_value:.2f}', transform=plt.gca().transAxes, color='red')
plt.text(1.02, 0.65, f'Mediana: {median_value}', transform=plt.gca().transAxes, color='green')
plt.text(1.02, 0.55, f'Máximo: {max_value}', transform=plt.gca().transAxes, color='orange')
plt.text(1.02, 0.45, f'Mínimo: {min_value}', transform=plt.gca().transAxes, color='purple')

plt.show()

"""Outliers de la variable de interés 'review_scores_rating'"""

sns.boxplot(x=df_train["review_scores_rating"])

Q1 = df_train["review_scores_rating"].quantile(0.25)
Q3 = df_train["review_scores_rating"].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR

df_train[(df_train["review_scores_rating"] < lower) | (df_train["review_scores_rating"] > upper)]

"""# Análisis de outliers

Boxplots
"""

copia2 = df_train[['host_total_listings_count', 'accommodates', 'bedrooms', 'number_of_reviews', 'review_scores_rating', 
                  'review_scores_cleanliness', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'review_scores_checkin',
                  ]]

plt.figure(figsize=(15, 10))
plt.subplots_adjust(hspace=0.5)

variables = copia2.columns
for i, variable in enumerate(variables):
    plt.subplot(4, 4, i+1)
    sns.boxplot(x=copia2[variable])
    plt.title(variable)
    plt.xlabel('')
    plt.ylabel('')

plt.tight_layout()
plt.show()

"""'host_total_listings_count'

Se asume que es posible que un host tenga 673 alojamientos, no se considera un error por lo tanto no se lo tratará como un valor atípico erróneo

# Evaluación de las columnas con missing values

**Columnas con missing values:** 'license', 'review_scores_value', 'review_scores_location', 'review_scores_communication', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'calendar_updated', 'bathrooms'
'bathrooms_text', 'bedrooms', 'beds', 'neighbourhood_group_cleansed', 'neighbourhood', 'host_about', 'host_response_time', 'host_response_rate','host_acceptance_rate', 'host_is_superhost', 'host_neighbourhood', 'host_location', 'neighborhood_overview', 'host_id' 
**A continuación se mostrará como se procedió con cada variable para el tratamiento de los valores faltantes**

Variable license
"""

print(df_train['license'].isnull().sum())
print(df_train['license'].nunique()) 
# En cuanto a los datos repetidos, que son 752 ((4928-98)-4078), se asumirá que no son erróneos sino que puede haber varios alojamientos con la misma licencia ya que esta pertenece al propietario y no al alojamiento

def encode_license(data):
    unique_licenses = data.unique()
    license_dict = {license: i+1 for i, license in enumerate(unique_licenses)}
    encoded_data = data.replace(license_dict)
    return encoded_data

"""Variables review_scores_value, review_scores_location, review_scores_communication, review_scores_accuracy, review_scores_cleanliness y review_scores_checkin"""

print(df_train['review_scores_value'].isnull().sum())
# En todos los casos es 4, se decidió reemplazar los valores faltantes por la media de la variable review_scores_value

"""Variable calendar_updated"""

print(df_train['calendar_updated'].isnull().sum())
# Todos los valores son nulos, es por esto que se decide eliminar la variable

"""Variable bathrooms"""

print(df_train['bathrooms'].isnull().sum())
# Todos los valores son nulos, es por esto que se decide eliminar la variable

"""Variable bathrooms_text"""

# La siguiente función limpia la variable 'bathrooms_text' y además por como se define la nueva variable 'cantidad_baños', en caso de que hubiera un missing value originalmente, este quedará como un 0 ya que se asume que el alojamiento no tiene baño.
def limpiar_baños(texto):
    cantidad_baños = 0

    if isinstance(texto, float):
        texto = str(texto)
    
    if ' bath' in texto:
        partes = texto.split(' ')
        cantidad_baños = float(partes[0])
    return cantidad_baños

# La siguiente función limpia la variable 'bathrooms_text' y además por como se define la nueva variable 'privacidad_baños', en caso de que hubiera un missing value originalmente, este quedará como "no info".
def extraer_privacidad_baño(texto):
    privacidad_baño = None
    
    if isinstance(texto, float):
        texto = str(texto)

    if 'shared' in texto:
        privacidad_baño = 'shared'
    elif 'private' in texto:
        privacidad_baño = 'private'
    else:
        privacidad_baño = 'no info'   
    return privacidad_baño

"""Variable 'bedrooms'"""

print(df_train['bedrooms'].isnull().sum())
# Se decidió reemplazar los valores faltantes por la media de la variable bedrooms

"""Variable 'beds'"""

print(df_train['beds'].isnull().sum())
# Se decidió reemplazar los valores faltantes por la media de la variable beds

"""Variable neighbourhood_group_cleansed"""

print(df_train['neighbourhood_group_cleansed'].isnull().sum())
# Todos los valores son nulos, es por esto que se decide eliminar la variable

"""Variable neighbourhood"""

print(df_train['neighbourhood'].isnull().sum())

# Limpieza de la variable neighbourhood
def limpiar_columna_neighbourhood(neighbourhood):
    ciudad = neighbourhood.apply(lambda x: x.split(';')[-1].split(',')[0].strip() if isinstance(x, str) else "")
    return ciudad

# Se puede ver que todos los alojamientos se encuentran en Holanda Septentrional, por lo tanto como no se considera de utilidad no se utilizarán estas variables

"""Variable neighbourhood_cleansed"""

print(df_train.neighbourhood_cleansed.unique())

diccionario_barrios = {
    'Centrum-Oost': 'Centrum',
    'Centrum-West': 'Centrum',
    'Oud-Oost': 'Oud-Oost',
    'De Pijp - Rivierenbuurt': 'De Pijp - Rivierenbuurt',
    'Noord-Oost': 'Noord',
    'Oud-Noord': 'Noord',
    'Noord-West': 'Noord',
    'De Aker - Nieuw Sloten': 'De Aker - Nieuw Sloten',
    'Geuzenveld - Slotermeer': 'Geuzenveld - Slotermeer',
    'Bijlmer-Centrum': 'Bijlmer-Centrum',
    'Buitenveldert - Zuidas': 'Buitenveldert - Zuidas',
    'Westerpark': 'Westerpark',
    'Slotervaart': 'Slotervaart',
    'De Baarsjes - Oud-West': 'De Baarsjes - Oud-West',
    'Bos en Lommer': 'Bos en Lommer',
    'IJburg - Zeeburgereiland': 'IJburg - Zeeburgereiland',
    'Watergraafsmeer': 'Watergraafsmeer',
    'Osdorp': 'Osdorp',
    'Gaasperdam - Driemond': 'Gaasperdam - Driemond',
    'Bijlmer-Oost': 'Bijlmer-Oost'
}

barrio = 'neighbourhood_cleansed'

# Como la nueva columna es 'barrios', se eliminará 'neighbourhood_cleansed'

"""Variable neighbourhood_overview"""

print(df_train['neighborhood_overview'].isnull().sum())

def get_sentiment_score(text):
    if isinstance(text, str):
        blob = TextBlob(text)
        return blob.sentiment.polarity
    else:
        return None

"""Variable description"""

print(df_train['description'].isnull().sum())

"""Variable host_about"""

print(df_train['host_about'].isnull().sum())
# Por la cantidad de valores faltantes y como no se pudo hacer un sentiment analysis exitoso para esta variable, se decidió eliminarla

"""Variable 'host_response_time'"""

print(df_train['host_response_time'].isnull().sum())

print(df_train.host_response_time.unique())

def limpiar_host_response_time(df, columna):
    mapeo_valores = {
        'within a day': 1,
        'within an hour': 0.04,
        'within a few hours': 0.25,
        'a few days or more': 72
    }

    df[columna] = df[columna].str.lower().str.strip()

    df['response_time'] = df[columna].map(mapeo_valores)
    promedio = df['response_time'].mean()
    df['response_time'].fillna(promedio, inplace=True)
    
    return df

columna_host_response_time = 'host_response_time'

# Ya que la variable 'host_response_time' ya no es de utilidad, se la eliminará

"""Variable 'host_response_rate'"""

def calculate_response_ratio(response_rate):
    if response_rate < 100:
        return 0
    else:
        return 1

"""Variable 'host_acceptance_rate'"""

print(df_train['host_acceptance_rate'].isnull().sum())
print(df_train.host_acceptance_rate.unique())

def limpiar_host_acceptance_rate(df, columna):

    df[columna] = df[columna].str.rstrip('%').astype(float) / 100.0
    
    media = df[columna].mean()
    
    df[columna] = df[columna].fillna(media)
    
    return df

columna_host_acceptance_rate = 'host_acceptance_rate'

"""Variable host_is_superhost"""

def limpiar_host_is_superhost(df, columna):

    if 'processed_' + columna in df.columns:
        return df

    df[columna] = df[columna].map({'f': 0, 't': 1})
    
    df[columna] = df[columna].fillna(0)
    
    df['processed_' + columna] = True
    
    return df

columna_host_is_superhost = 'host_is_superhost'

print(df_train['host_is_superhost'].isnull().sum())
print(df_train.host_is_superhost.unique())

"""Variable host_neighbourhood"""

print(df_train.host_neighbourhood.unique())
# No hay ningún nombre de barrio que referencie a otro barrio de la lista, por lo tanto no se hará un diccionario
print(df_train['host_neighbourhood'].isnull().sum())
# Hay muchos valores null, por lo tanto se eliminará la variable

"""Variable 'host_location'"""

print(df_train.host_location.unique())
print(df_train.host_location.isnull().sum())
# Como ningún nombre de barrio hace referencia a otro barrio de la lista, no se hará un diccionario

"""Variable neighborhood_overview"""

print(df_train.neighborhood_overview.unique())
print(df_train.neighborhood_overview.isnull().sum())
# Por la cantidad de valores faltantes y como no se pudo hacer un sentiment analysis exitoso para esta variable, se decidió eliminarla

"""Variable price"""

def limpiar_precios(texto):
    if isinstance(texto, str):
        texto_limpio = texto.replace('$', '').replace(',', '').replace(' ', '')

        precio_float = float(texto_limpio)
        return precio_float
    else:
        return float(texto)

"""Variable amenities"""

print(df_train.amenities.unique())
print(df_train.amenities.isnull().sum())

"""# Cambios aplicados sobre la base de entrenamiento en base a los missing values"""

df_train['license_encoded'] = encode_license(df_train['license'])

df_train['cantidad_baños'] = df_train['bathrooms_text'].apply(limpiar_baños)
df_train['privacidad_baño'] = df_train['bathrooms_text'].apply(extraer_privacidad_baño)

df_train['barrios'] = df_train[barrio].replace(diccionario_barrios)

df_train['sentiment_score_neighborhood'] = df_train['neighborhood_overview'].apply(lambda x: get_sentiment_score(x))

df_train['sentiment_score_description'] = df_train['description'].apply(lambda x: get_sentiment_score(x))

df_train = limpiar_host_response_time(df_train, columna_host_response_time)

df_train['host_response_rate'] = df_train['host_response_rate'].astype(str).str.replace('%', '').astype(float)

df_train['response_ratio'] = df_train['host_response_rate'].apply(calculate_response_ratio)

response_ratio_percentage = (df_train['response_ratio'].sum() / len(df_train)) * 100
print("Porcentaje de anfitriones con ratio de respuesta alto: {:.2f}%".format(response_ratio_percentage))

df_train = limpiar_host_acceptance_rate(df_train, columna_host_acceptance_rate)

df_train = limpiar_host_is_superhost(df_train, columna_host_is_superhost)

df_train['price_clean'] = df_train['price'].apply(limpiar_precios)

"""# Cambios aplicados sobre la base de evaluación en base a los missing values

A continuación se realizarán en df_val todas las modificaciones realizadas a df_train
"""

df_val['license_encoded'] = encode_license(df_val['license'])

df_val['cantidad_baños'] = df_val['bathrooms_text'].apply(limpiar_baños)
df_val['privacidad_baño'] = df_val['bathrooms_text'].apply(extraer_privacidad_baño)

df_val['barrios'] = df_val[barrio].replace(diccionario_barrios)

df_val['sentiment_score_neighborhood'] = df_val['neighborhood_overview'].apply(lambda x: get_sentiment_score(x))

df_val['sentiment_score_description'] = df_val['description'].apply(lambda x: get_sentiment_score(x))

df_val = limpiar_host_response_time(df_val, columna_host_response_time)

df_val['host_response_rate'] = df_val['host_response_rate'].astype(str).str.replace('%', '').astype(float)

df_val['response_ratio'] = df_val['host_response_rate'].apply(calculate_response_ratio)

df_val = limpiar_host_acceptance_rate(df_val, columna_host_acceptance_rate)

df_val = limpiar_host_is_superhost(df_val, columna_host_is_superhost)

df_val['price_clean'] = df_val['price'].apply(limpiar_precios)

"""# Imputación de missing values"""

from sklearn.impute import SimpleImputer
def preprocesar_datos(df, train):
       
    # Rellenar campos vacíos con 0 o "No info"
    df.host_location.fillna("No info", inplace = True)
    df.host_is_superhost.fillna(0, inplace = True)

    # Variables review_scores_...
    media1 = train['review_scores_accuracy'].mean()
    df['review_scores_accuracy'].fillna(media1, inplace=True)
    media2 = train['review_scores_cleanliness'].mean()
    df['review_scores_cleanliness'].fillna(media2, inplace=True)
    media3 = train['review_scores_checkin'].mean()
    df['review_scores_checkin'].fillna(media3, inplace=True)
    media4 = train['review_scores_communication'].mean()
    df['review_scores_communication'].fillna(media4, inplace=True)
    media5 = train['review_scores_location'].mean()
    df['review_scores_location'].fillna(media5, inplace=True)
    media6 = train['review_scores_value'].mean()
    df['review_scores_value'].fillna(media6, inplace=True)

    # Variable bedrooms
    media7 = train['bedrooms'].mean()
    df['bedrooms'].fillna(media7, inplace=True)
    # Variable beds
    media8 = train['beds'].mean()
    df['beds'].fillna(media8, inplace=True)
    # Variables neighborhood_overview y description
    media9 = train['sentiment_score_neighborhood'].mean()
    df['sentiment_score_neighborhood'].fillna(media9, inplace=True)
    media10 = train['sentiment_score_description'].mean()
    df['sentiment_score_description'].fillna(media10, inplace=True)
    
    # Eliminar las columnas totalmente vacías o que no sirven
    df.drop(['license','description','price','calendar_updated','bathrooms','bathrooms_text','neighbourhood_group_cleansed', 'neighbourhood', 'neighbourhood_cleansed', 'host_about', 'host_response_time', 'host_neighbourhood', 'host_response_rate', 'neighborhood_overview'], axis = 1, inplace = True)
    
    return df

"""# Aplicación de preprocesar_datos a df_train y df_val

Nueva base 'clean_df', que reemplaza a df_train
"""

clean_df = preprocesar_datos(df_train, df_train)

"""Nueva base 'clean_df_val', que reemplaza a df_train"""

clean_df_val = preprocesar_datos(df_val, df_train)

"""Se chequea que la única columna distinta sea 'review_scores_rating'"""

def comparar_columnas(df1, df2):
    columnas_df1 = set(df1.columns)
    columnas_df2 = set(df2.columns)
    
    columnas_no_comunes = columnas_df1.symmetric_difference(columnas_df2)
    
    return columnas_no_comunes

columnas_no_comunes = comparar_columnas(clean_df, clean_df_val)
print(columnas_no_comunes)

num_registros = len(clean_df)
num_variables = len(clean_df.columns)
num_registros1 = len(clean_df_val)
num_variables1 = len(clean_df_val.columns)

print("Número de registros clean_df:", num_registros)
print("Número de variables clean_df:", num_variables)
print("Número de registros clean_df_val:", num_registros)
print("Número de variables clean_df_val:", num_variables)

"""# Análisis de correlación

Análisis de correlación (luego de la limpieza)
"""

copia = clean_df[['response_ratio', 'host_total_listings_count', 'accommodates', 'bedrooms', 'number_of_reviews', 'review_scores_rating', 
                  'review_scores_cleanliness', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'review_scores_checkin',
                  'total_comm', 'superhost_experience', 'response_ratio', 'cantidad_baños', 'price_clean']]

copia.corr()

"""Heatmap"""

mtx = copia.corr()
mask = np.triu(np.ones_like(mtx, dtype=bool))
sns.set_context('notebook', font_scale=0.9)
fig, ax = plt.subplots(figsize=(16, 8))
sns.heatmap(ax=ax, data=mtx, annot=True, fmt='.2f', cmap='coolwarm', mask=mask)

from sklearn.linear_model import Lasso

alphas = [0.01, 0.1, 1.0, 10.0]

for alpha in alphas:
    lasso_model = Lasso(alpha=alpha)
    lasso_model.fit(X_train, y_train)
    y_pred = lasso_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Alpha: {alpha}")
    print('MSE:', mse)
    print('R²:', r2)
    print("---")

best_alpha = alphas[np.argmax(r2_score)]
best_lasso_model = Lasso(alpha=best_alpha)
best_lasso_model.fit(X_train, y_train)
y_pred = best_lasso_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mejor modelo Lasso:")
print('MSE:', mse)
print('R²:', r2)

"""# Creación de variables en df_train (mas allá de los missing values)

Años de experiencia del host
"""

from datetime import datetime

clean_df['host_since'] = pd.to_datetime(clean_df['host_since'])
current_date = datetime.now()
clean_df['superhost_experience'] = (current_date - clean_df['host_since']).dt.days / 365
promedio_superhost_experience = clean_df['superhost_experience'].mean()
clean_df['host_since'] = pd.to_numeric(clean_df['host_since'])

print("Superhost Experience promedio: {:.2f} años".format(promedio_superhost_experience))

"""Commodities totales del alojamiento"""

clean_df['total_comm'] = clean_df['amenities'].str.count(',') + 1
promedio_comodidades_totales = clean_df['total_comm'].mean()

print("Promedio de Comodidades Totales por alojamiento: {:.2f}".format(promedio_comodidades_totales))

"""Comentarios totales"""

clean_df['total_comments'] = clean_df['number_of_reviews'] + clean_df['number_of_reviews_l30d']

clean_df.total_comments.fillna(0, inplace = True)

"""Rango estadía"""

clean_df['stay_range'] = clean_df['maximum_nights'] - clean_df['minimum_nights']

"""# Creación de las mismas variables en df_val

Años de experiencia del host
"""

clean_df_val['host_since'] = pd.to_datetime(clean_df_val['host_since'])
current_date = datetime.now()
clean_df_val['superhost_experience'] = (current_date - clean_df_val['host_since']).dt.days / 365
promedio_superhost_experience = clean_df_val['superhost_experience'].mean()
clean_df_val['host_since'] = pd.to_numeric(clean_df_val['host_since'])

print("Superhost Experience promedio: {:.2f} años".format(promedio_superhost_experience))

"""Commodities totales del alojamiento"""

clean_df_val['total_comm'] = clean_df_val['amenities'].astype(str).str.count(',') + 1
promedio_comodidades_totales = clean_df_val['total_comm'].mean()

print("Promedio de Comodidades Totales por alojamiento: {:.2f}".format(promedio_comodidades_totales))

"""Comentarios totales"""

clean_df_val['total_comments'] = clean_df_val['number_of_reviews'] + clean_df_val['number_of_reviews_l30d']

clean_df_val.total_comments.fillna(0, inplace = True)

"""Rango estadía"""

clean_df_val['stay_range'] = clean_df_val['maximum_nights'] - clean_df['minimum_nights']

"""# Transformación de columnas

A continuación se mostrarán las modificaciones finales a las bases clean_df y clean_df_val previas a la evaluación de modelos

Modificaciones sobre clean_df
"""

clean_df.drop('host_since', axis=1, inplace=True)
clean_df.drop('processed_host_is_superhost', axis=1, inplace=True)

clean_df.drop('amenities', axis=1, inplace=True)

clean_df.drop('first_review', axis=1, inplace=True)
clean_df.drop('last_review', axis=1, inplace=True)

clean_df.drop('name', axis=1, inplace=True)
clean_df.drop('host_name', axis=1, inplace=True)
clean_df.drop('host_location', axis=1, inplace=True)
clean_df.drop('calendar_last_scraped', axis=1, inplace=True)

def convert_source_value(value):
    if value == 'city scrape':
        return 0
    elif value == 'previous scrape':
        return 1
    else:
        return value

clean_df['source'] = clean_df['source'].apply(convert_source_value)

def transform_host_verifications(df):
    df['ver_sum'] = df['host_verifications'].apply(lambda x: len(x.split(',')) if isinstance(x, str) else 0)
    return df

clean_df = transform_host_verifications(clean_df)
print(clean_df['ver_sum'])

clean_df['ver_sum'].fillna(0, inplace=True)

clean_df.drop('host_verifications', axis=1, inplace=True)

"""Modificaciones sobre clean_df_val"""

clean_df_val.drop('host_since', axis=1, inplace=True)
clean_df_val.drop('processed_host_is_superhost', axis=1, inplace=True)

clean_df_val.drop('amenities', axis=1, inplace=True)

clean_df_val['source'] = clean_df_val['source'].apply(convert_source_value)

clean_df_val.drop('first_review', axis=1, inplace=True)
clean_df_val.drop('last_review', axis=1, inplace=True)

clean_df_val.drop('name', axis=1, inplace=True)
clean_df_val.drop('host_name', axis=1, inplace=True)
clean_df_val.drop('host_location', axis=1, inplace=True)
clean_df_val.drop('calendar_last_scraped', axis=1, inplace=True)

clean_df_val = transform_host_verifications(clean_df_val)

clean_df_val['ver_sum'].fillna(0, inplace=True)

clean_df_val.drop('host_verifications', axis=1, inplace=True)

"""# Label Enconder

clean_df
"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

columns = ['host_has_profile_pic','host_identity_verified','has_availability','property_type','privacidad_baño','room_type','privacidad_baño', 'barrios', 'response_time',
           'instant_bookable']
for i in columns:
    clean_df[i] = le.fit_transform(clean_df[i])

X = clean_df.drop(['review_scores_rating'], axis=1)
y = clean_df['review_scores_rating']

"""clean_df_val"""

columns = ['host_has_profile_pic','host_identity_verified','has_availability','property_type','privacidad_baño','room_type','privacidad_baño', 'barrios', 'response_time',
           'instant_bookable']
for i in columns:
    clean_df_val[i] = le.fit_transform(clean_df_val[i])

"""# Modelos predictivos

Partición de la base
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 21)

"""70% train y 30% test"""

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Árbol con Grid Search (MSE = 0.0252 Y R² = 0.4728)"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=DecisionTreeRegressor(random_state=42), param_grid=param_grid, scoring='r2', cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

arbol = DecisionTreeRegressor(random_state=42, **best_params)
arbol.fit(X_train, y_train)

y_pred = arbol.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Árbol de decisión (MSE: 0.0260 y R²: 0.3820)"""

from sklearn.metrics import mean_squared_error

arbol = DecisionTreeRegressor(random_state=42)

arbol.fit(X_train, y_train)

y_pred = arbol.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Random Forest (MSE = 0.0161 y R² = 0.6619)"""

random_forest = RandomForestRegressor(n_estimators=100, random_state=42)
random_forest.fit(X_train, y_train)

y_pred_rf = random_forest.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
print("MSE (Random Forest):", mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print('R² (Random Forest):', r2_rf)

"""Feature importance"""

feature_importance = arbol.feature_importances_

for feature, importance in zip(X.columns, feature_importance):
    print(feature, importance)

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor()
rf_model.fit(X_train, y_train)

feature_importances = rf_model.feature_importances_

plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances)
plt.xticks(range(len(feature_importances)), X_train.columns, rotation='vertical')
plt.xlabel('Características')
plt.ylabel('Importancia')
plt.title('Importancia de características en Random Forest')
plt.show()

"""Regresión lineal simple (MSE: 0.0411 y R²: 0.1409)"""

linear_model = LinearRegression()

linear_model.fit(X_train, y_train)

y_pred = linear_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Catboost (MSE: 0.0136 y R²: 0.7154)"""

catboost_model = CatBoostRegressor()

catboost_model.fit(X_train, y_train)

y_pred = catboost_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Catboost ganador (MSE: 0.0126 y R²: 0.7369)"""

params = {
    'learning_rate': 0.1,
    'depth': 8,
    'l2_leaf_reg': 4,
    'iterations': 100,
    'colsample_bylevel': 0.9 }

catboost_model_5 = CatBoostRegressor(**params)

catboost_model_5.fit(X_train, y_train)

y_pred = catboost_model_5.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)

r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Catboost (MSE: 0.0135 y R²: 0.7177)"""

X_train_filtered = X_train.drop(['id', 'latitude', 'longitude','minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
       'maximum_minimum_nights', 'minimum_maximum_nights', 'has_availability','number_of_reviews', 'number_of_reviews_ltm',
       'number_of_reviews_l30d'], axis=1)
X_test_filtered = X_test.drop(['id', 'latitude', 'longitude','minimum_nights', 'maximum_nights', 'minimum_minimum_nights',
       'maximum_minimum_nights', 'minimum_maximum_nights', 'has_availability','number_of_reviews', 'number_of_reviews_ltm', 
       'number_of_reviews_l30d'], axis=1)

catboost_model_3 = CatBoostRegressor()

catboost_model_3.fit(X_train_filtered, y_train)

y_pred = catboost_model_3.predict(X_test_filtered)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('MSE:', mse)
print('R²:', r2)

"""Catboost 4 (MSE: 0.0132 y R²: 0.7232)"""

catboost_model_4 = CatBoostRegressor()

catboost_model_4.fit(X_train, y_train)

feature_importance = catboost_model_4.feature_importances_

feature_names = X_train.columns

sorted_indices = np.argsort(feature_importance)[::-1]
sorted_features = feature_names[sorted_indices]

for feature, importance in zip(sorted_features, feature_importance[sorted_indices]):
    print(f'{feature}: {importance}')

n_features_to_keep = 35
selected_features = sorted_features[:n_features_to_keep]

X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

catboost_model_4.fit(X_train_selected, y_train)

y_pred = catboost_model_4.predict(X_test_selected)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print('MSE:', mse)

r2 = r2_score(y_test, y_pred)
print('R²:', r2)

from sklearn.model_selection import cross_val_score
from catboost import CatBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score

catboost_model_2 = CatBoostRegressor()

cv_scores = cross_val_score(catboost_model_2, X_train, y_train, cv=5, scoring='neg_mean_squared_error')

mse_mean = -cv_scores.mean()
r2_mean = cv_scores.mean()

catboost_model_2.fit(X_train, y_train)

y_pred = catboost_model_2.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('Mean MSE (Cross Validation):', mse_mean)
print('MSE (Test Set):', mse)
print('Mean R² (Cross Validation):', r2_mean)
print('R² (Test Set):', r2)

"""Random Forest con Grid Search (R² = 0.6767)"""

rf_model = RandomForestRegressor(max_depth=10, n_estimators=150)

param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}

grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='r2', cv=5)

grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_rf_model = RandomForestRegressor(**best_params)
best_rf_model.fit(X_train, y_train)

y_pred = best_rf_model.predict(X_test)

r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Gradient Boosting con Grid Search (MSE: 0.0157 y R²: 0.6705)"""

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

gb_model = GradientBoostingRegressor()

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, scoring='r2', cv=5)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_score = grid_search.best_score_

best_gb_model = GradientBoostingRegressor(**best_params)
best_gb_model.fit(X_train, y_train)

y_pred = best_gb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Catboost con GridSearch (MSE: 0.0159 y R²: 0.6660)"""

param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'depth': [4, 6, 8],
    'n_estimators': [100, 200, 300]
}

catboost_model = CatBoostRegressor()

grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, cv=3, scoring='r2')
grid_search.fit(X_train, y_train)

best_catboost_model = grid_search.best_estimator_

y_pred = best_catboost_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print('MSE:', mse)
print('R²:', r2)
print('Mejores hiperparámetros:', grid_search.best_params_)

"""Optimización bayesiana (R²: 0.6457)"""

import optuna
def objective(trial):
    params = {
        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 300]),
        'max_depth': trial.suggest_int('max_depth', 3, 7),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),
        'subsample': trial.suggest_uniform('subsample', 0.8, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.8, 1.0)
    }
    
    lgb_model = LGBMRegressor(**params)
    lgb_model.fit(X_train, y_train)
    
    y_pred = lgb_model.predict(X_test)
    
    r2 = r2_score(y_test, y_pred)
    return r2

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

best_params = study.best_params
best_lgb_model = LGBMRegressor(**best_params)
best_lgb_model.fit(X_train, y_train)

y_pred = best_lgb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Random forest con estandarización (MSE: 0.0173 y R²: 0.6373)"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


y_train = y_train.values.reshape(-1, 1)
y_test = y_test.values.reshape(-1, 1)    

rf_model = RandomForestRegressor(max_depth=5, n_estimators=100)
rf_model.fit(X_train_scaled, y_train)

y_pred = rf_model.predict(X_test_scaled)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""RandomizedSearchCV (R²: 0.6217)"""

from sklearn.model_selection import RandomizedSearchCV
lgb_model = LGBMRegressor()

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.05, 0.01],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0]
}

random_search = RandomizedSearchCV(estimator=lgb_model, param_distributions=param_grid, scoring='r2', cv=5, n_iter=10, random_state=42)
random_search.fit(X_train, y_train)

best_params = random_search.best_params_
best_score = random_search.best_score_

best_lgb_model = LGBMRegressor(**best_params)
best_lgb_model.fit(X_train, y_train)

y_pred = best_lgb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""LightGBM (MSE: 0.0185 y R²: 0.6123)"""

lgb_model = lgb.LGBMRegressor()

lgb_model.fit(X_train, y_train)

y_pred = lgb_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Regresión lineal con Lasso (MSE: 0.0188 y R²: 0.6068)"""

from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.01) 
lasso_model.fit(X_train, y_train)

y_pred = lasso_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""XGBRegressor (MSE = 0.0196 y R² = 0.5896)"""

model = xgb.XGBRegressor()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Variando los parámetros de la regresión lineal con Ridge (MSE: 0.0229 y R²: 0.5214)"""

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

ridge_model = Ridge(alpha=0.5)

ridge_model.fit(X_train, y_train)

y_pred = ridge_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Regresión lineal con Ridge (MSE: 0.0229 y R²: 0.5201)"""

from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=0.1) 
ridge_model.fit(X_train, y_train)

y_pred = ridge_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print('MSE:', mse)
r2 = r2_score(y_test, y_pred)
print('R²:', r2)

"""Variando los parámetros de la regresión lineal con Lasso (MSE: 0.0254 y R²: 0.5089)

# Exportación

Catboost
"""

df_val = pd.read_csv('/content/base_val.csv')

X_val = clean_df_val
y_pred1 = catboost_model.predict(X_val)

df_pred = pd.DataFrame({"id": df_val["id"], "review_scores_rating": y_pred1})
print(df_pred.dtypes.unique())
print(df_pred)

df_pred.to_csv("modelo_cb1.csv", index=False)

X_val = clean_df_val
y_pred1 = catboost_model_4.predict(X_val)

df_pred = pd.DataFrame({"id": df_val["id"], "review_scores_rating": y_pred1})
print(df_pred.dtypes.unique())
print(df_pred)

df_pred.to_csv("modelo_cb4.csv", index=False)

y_pred1_series = pd.Series(y_pred1)

# Obtener resumen estadístico
summary = y_pred1_series.info()

print(summary)
print(y_pred1)

X_val = clean_df_val
y_pred = best_lasso_model.predict(X_val)

df_pred = pd.DataFrame({"id": df_val["id"], "review_scores_rating": y_pred})


df_pred.to_csv("prediccion_lasso.csv", index=False)

X_val = clean_df_val

y_pred = ridge_model.predict(X_val)

df_pred = pd.DataFrame({"id": df_val["id"].astype('int64'), "review_scores_rating": y_pred})

df_pred.to_csv("prediccion_2.csv", index=False)

X_val = clean_df_val
y_pred = catboost_model_5.predict(X_val)

df_results = pd.DataFrame({'id': df_val['id'], 'review_scores_rating': y_pred})

print(df_results.dtypes.unique())
print(df_results)

df_results.to_csv('results_cb_5.csv', index=False)